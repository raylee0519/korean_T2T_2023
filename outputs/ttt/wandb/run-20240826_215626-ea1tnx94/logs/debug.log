2024-08-26 21:56:26,165 INFO    MainThread:17376 [wandb_setup.py:_flush():68] Configure stats pid to 17376
2024-08-26 21:56:26,165 INFO    MainThread:17376 [wandb_setup.py:_flush():68] Loading settings from C:\Users\User\.config\wandb\settings
2024-08-26 21:56:26,165 INFO    MainThread:17376 [wandb_setup.py:_flush():68] Loading settings from C:\Users\User\mygithub\korean_T2T_2023\wandb\settings
2024-08-26 21:56:26,165 INFO    MainThread:17376 [wandb_setup.py:_flush():68] Loading settings from environment variables: {'_require_service': 'True'}
2024-08-26 21:56:26,166 WARNING MainThread:17376 [wandb_setup.py:_flush():68] Could not find program at -m run.__main__
2024-08-26 21:56:26,166 INFO    MainThread:17376 [wandb_setup.py:_flush():68] Inferring run settings from compute environment: {'program_relpath': None, 'program': '-m run.__main__'}
2024-08-26 21:56:26,166 INFO    MainThread:17376 [wandb_init.py:_log_setup():476] Logging user logs to outputs/ttt\wandb\run-20240826_215626-ea1tnx94\logs\debug.log
2024-08-26 21:56:26,166 INFO    MainThread:17376 [wandb_init.py:_log_setup():477] Logging internal logs to outputs/ttt\wandb\run-20240826_215626-ea1tnx94\logs\debug-internal.log
2024-08-26 21:56:26,166 INFO    MainThread:17376 [wandb_init.py:init():516] calling init triggers
2024-08-26 21:56:26,166 INFO    MainThread:17376 [wandb_init.py:init():519] wandb.init called with sweep_config: {}
config: {}
2024-08-26 21:56:26,167 INFO    MainThread:17376 [wandb_init.py:init():569] starting backend
2024-08-26 21:56:26,167 INFO    MainThread:17376 [wandb_init.py:init():573] setting up manager
2024-08-26 21:56:26,169 INFO    MainThread:17376 [backend.py:_multiprocessing_setup():102] multiprocessing start_methods=spawn, using: spawn
2024-08-26 21:56:26,171 INFO    MainThread:17376 [wandb_init.py:init():580] backend started and connected
2024-08-26 21:56:26,173 INFO    MainThread:17376 [wandb_init.py:init():658] updated telemetry
2024-08-26 21:56:26,197 INFO    MainThread:17376 [wandb_init.py:init():693] communicating run to backend with 60 second timeout
2024-08-26 21:56:27,080 INFO    MainThread:17376 [wandb_run.py:_on_init():2000] communicating current version
2024-08-26 21:56:27,151 INFO    MainThread:17376 [wandb_run.py:_on_init():2004] got version response upgrade_message: "wandb version 0.17.7 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"

2024-08-26 21:56:27,151 INFO    MainThread:17376 [wandb_init.py:init():728] starting run threads in backend
2024-08-26 21:56:27,291 INFO    MainThread:17376 [wandb_run.py:_console_start():1980] atexit reg
2024-08-26 21:56:27,292 INFO    MainThread:17376 [wandb_run.py:_redirect():1838] redirect: 3
2024-08-26 21:56:27,292 INFO    MainThread:17376 [wandb_run.py:_redirect():1903] Wrapping output streams.
2024-08-26 21:56:27,292 INFO    MainThread:17376 [wandb_run.py:_redirect():1925] Redirects installed.
2024-08-26 21:56:27,292 INFO    MainThread:17376 [wandb_init.py:init():765] run started, returning control to user process
2024-08-26 21:56:27,543 INFO    MainThread:17376 [wandb_run.py:_config_callback():1160] config_cb None None {'vocab_size': 30000, 'max_position_embeddings': 1026, 'd_model': 768, 'encoder_ffn_dim': 3072, 'encoder_layers': 6, 'encoder_attention_heads': 16, 'decoder_ffn_dim': 3072, 'decoder_layers': 6, 'decoder_attention_heads': 16, 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'activation_function': 'gelu', 'init_std': 0.02, 'encoder_layerdrop': 0.0, 'decoder_layerdrop': 0.0, 'classifier_dropout': 0.1, 'use_cache': True, 'num_hidden_layers': 6, 'scale_embedding': False, 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'None', 'use_bfloat16': False, 'tf_legacy_loss': False, 'tie_word_embeddings': True, 'is_encoder_decoder': True, 'is_decoder': False, 'cross_attention_hidden_size': 'None', 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': 'None', 'num_return_sequences': 1, 'chunk_size_feed_forward': 0, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': 'None', 'forced_eos_token_id': 1, 'remove_invalid_values': False, 'exponential_decay_length_penalty': 'None', 'suppress_tokens': 'None', 'begin_suppress_tokens': 'None', 'architectures': ['BartModel'], 'finetuning_task': 'None', 'id2label/0': 'NEGATIVE', 'id2label/1': 'POSITIVE', 'label2id/NEGATIVE': 0, 'label2id/POSITIVE': 1, 'tokenizer_class': 'PreTrainedTokenizerFast', 'prefix': 'None', 'bos_token_id': 1, 'pad_token_id': 3, 'eos_token_id': 1, 'sep_token_id': 'None', 'decoder_start_token_id': 1, 'task_specific_params': 'None', 'problem_type': 'None', '_name_or_path': 'gogamza/kobart-base-v2', 'transformers_version': '4.23.1', 'add_bias_logits': False, 'add_final_layer_norm': False, 'classif_dropout': 0.1, 'do_blenderbot_90_layernorm': False, 'extra_pos_embeddings': 2, 'force_bos_token_to_be_generated': False, 'gradient_checkpointing': False, 'model_type': 'bart', 'normalize_before': False, 'normalize_embedding': True, 'static_position_embeddings': False, 'author': 'Heewon Jeon(madjakarta@gmail.com)', 'kobart_version': 2.0, 'total_steps': 44820, 'max_learning_rate': 0.0002, 'min_learning_rate': 1e-05, 'warmup_rate': 0.1}
